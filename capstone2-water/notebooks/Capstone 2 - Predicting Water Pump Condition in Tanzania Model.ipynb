{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone 2 - Predicting Water Pump Condition in Tanzania Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kenneth Liao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UN publishes and reviews a list of least developed countries (LDC) every 3 years. LDCs are “low-income countries confronting severe structural impediments to sustainable development. They are highly vulnerable to economic and environmental shocks and have low levels of human assets.”$^{1}$. Tanzania has been classified as an LDC since the UN published the first list of LDCs in 1971$^{2}$. A common challenge of LDCs is a lack of infrastructure to support the development of the nation, including access to education and healthcare, waste management, and potable water.\n",
    "\n",
    "According to UNICEF, as of 2017, more than 24 million Tanzanians lacked access to basic drinking water$^{3}$. This corresponds to only 56.7% of the country’s population having access to basic drinking water. Outside of developed urban areas, much of the potable water is accessed via water pumps. \n",
    "\n",
    "Taarifa is an open-source platform for crowd-sourced reporting and triaging of infrastructure related issues. Together with the Tanzanian Ministry of Water, data has been collected for thousands of water pumps throughout Tanzania. The goal of this project is to be able to predict the condition of these water pumps to improve maintenance, reduce pump downtime, and ensure basic water access for tens of millions of Tanzanians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "1. https://www.un.org/development/desa/dpad/least-developed-country-category.html\n",
    "2. https://www.un.org/development/desa/dpad/wp-content/uploads/sites/45/publication/ldc_list.pdf\n",
    "3. https://washwatch.org/en/countries/tanzania/summary/statistics/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the operating condition of water pumps in Tanzania given various metadata on each water pump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy will be to implement a Random Forest model for multiclass classification of the state of water pumps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is provided by Taarifa, together with the Tanzanian Ministry of Water and is hosted by DrivenData.org:\n",
    "\n",
    "https://www.drivendata.org/competitions/7/pump-it-up-data-mining-the-water-table/page/23/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the necessary libraries and the cleaned datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import iplot, plot, init_notebook_mode\n",
    "from config import credentials\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, confusion_matrix, f1_score, recall_score, accuracy_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle('../data/X_train.pkl')\n",
    "X_test = pd.read_pickle('../data/X_cv.pkl')\n",
    "y_train = pd.read_pickle('../data/y_train.pkl')\n",
    "y_test = pd.read_pickle('../data/y_cv.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {0: 'functional', 1: 'functional needs repair', 2: 'non functional'}\n",
    "labels_list = list(labels.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-box Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll start by building a baseline for which we can compare our model's results to. Recall that the majority class was **functional** which comprised 54.3% of the data. Let's see what the precision, recall, and f1-score metrics would look like for an out-of-box random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# define and train the model\n",
    "model = RandomForestClassifier(n_jobs=-1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predicted labels from the model\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the confusion matrix for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = pd.MultiIndex.from_tuples(('Actual', i) for i in labels_list)\n",
    "rows = pd.MultiIndex.from_tuples(('Predicted', i) for i in labels_list)\n",
    "cm = pd.DataFrame(confusion_matrix(y_test, y_pred), index=rows, columns=cols)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we can calculate the precision, recall, and f1-score of the model. From this summary it's easy to see that the majority of functional pumps were correctly classified as being functional. We were less accurate in correctly classifying the non functional pumps and even worse at correctly classifying the functional pumps needing repair. Let's summarize this by compute the precision, recall, and f1 scores for this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(y_test, y_pred):\n",
    "    scores = pd.DataFrame({'precision': precision_score(y_test, y_pred, average=None),\n",
    "             'recall': recall_score(y_test, y_pred, average=None),\n",
    "            'f1-score': f1_score(y_test, y_pred, average=None)},\n",
    "            index=labels_list).T\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a moment to interpret these scores and understand what is most important for our problem statement. First, some definitions:\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation*}\n",
    "Precision = \\frac{True Positive}{(True Positive + False Positive)}\n",
    "\\end{equation*}\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation*}\n",
    "Recall = \\frac{True Positive}{(True Positive + False Negative)}\n",
    "\\end{equation*}\n",
    "\n",
    "<br>\n",
    "\n",
    "\\begin{equation*}\n",
    "F1 = \\frac{2 * (Precision * Recall)}{(Precision + Recall)}\n",
    "\\end{equation*}\n",
    "\n",
    "<br>\n",
    "\n",
    "We're interested in predicting which pumps are functioning normally, which pumps are functioning but need to be repaired, and which pumps are completely non functioning. If a pump is non functional, it requires immediate attention as the population dependent on that water source cannot access clean water. Therefore, it's most critical that we predict this class with high recall. That is, for non functional pumps, we want to minimize the number of pumps we classify as being functional when they are actually non functional (false negatives). Of course if we took this to the extreme and assumed all pumps are non functional, we would have perfect recall but very low precision. This would be impractical because we would have to essentially send surveyors to every pump anyway to check their status, in which case the model is useless. With this in mind, the next step is to try to optimize this model to improve the recall of the non functional group without lowing too much precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning & Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For hyperparameter tuning, we'll turn to GridSearchCV. To optimize the model, I want to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "clf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "\n",
    "param_grid = {'n_estimators':[10,100]}\n",
    "\n",
    "scorers = {'Precision': make_scorer(precision_score),\n",
    "          'Recall': make_scorer(recall_score),\n",
    "          'F1_score': make_scorer(f1_score)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(model, param_grid, scorers):\n",
    "    # define the stratified k-fold model\n",
    "    skf = StratifiedKFold(n_splits=5)\n",
    "    \n",
    "    # define the grid_search model\n",
    "    gs = GridSearchCV(model, param_grid, scoring=scorers, refit=False,\n",
    "                           cv=skf, return_train_score=True, n_jobs=-1)\n",
    "    \n",
    "    # train the model\n",
    "    gs.fit(X_train.values, y_train.values)\n",
    "    \n",
    "    # make predictions on the test set\n",
    "    y_pred = grid_search.predict(X_test.values)\n",
    "    \n",
    "    print('Best params for {}'.format(refit_score))\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_search(clf, param_grid, scorers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
