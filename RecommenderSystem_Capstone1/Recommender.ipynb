{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# enable offline plotting in plotly\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our 3 datasets\n",
    "users = pd.read_csv('data/user_features.csv')\n",
    "problems =  pd.read_csv('data/problem_features.csv')\n",
    "submissions = pd.read_csv('data/train_submissions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, R_test = train_test_split(submissions, test_size=0.25, random_state=42)\n",
    "\n",
    "R_train, R_cv = train_test_split(train, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "R_train = R_train.set_index(['user_id','problem_id']).unstack(level=-1)\n",
    "R_cv = R_cv.set_index(['user_id','problem_id']).unstack(level=-1)\n",
    "\n",
    "R_train.columns = R_train.columns.droplevel()\n",
    "R_cv.columns = R_cv.columns.droplevel()\n",
    "\n",
    "empty_sub = pd.DataFrame(np.nan, index=users.user_id.unique(), \n",
    "                         columns=problems.problem_id.unique())\n",
    "\n",
    "R_train = empty_sub.fillna(R_train)\n",
    "R_cv = empty_sub.fillna(R_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('results/'+ name + '.pkl', 'wb') as f:\n",
    "        pkl.dump(obj, f, pkl.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('results/' + name + '.pkl', 'rb') as f:\n",
    "        return pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unroll(M_users, M_items):\n",
    "    \n",
    "    \"\"\"Reshape 2 matrices into a single 1D array. \n",
    "    Inverse function of `roll`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    M_users : 2D numpy array\n",
    "        Matrix of user latent features. Has\n",
    "        dimensions (n_users, n_features).\n",
    "    M_items : 2D numpy array\n",
    "        Matrix of item latent features. Has\n",
    "        dimensions (n_items, n_features).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x_users_items : 1D numpy array\n",
    "        User and item latent features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert matrices to np arrays\n",
    "    M_users = np.array(M_users)\n",
    "    M_items = np.array(M_items)\n",
    "\n",
    "    # flatten 2D arrays into 1D arrays\n",
    "    x_users = M_users.flatten(order='C')\n",
    "    x_items = M_items.flatten(order='C')\n",
    "    \n",
    "    # concatenate user and item 1D arrays\n",
    "    x_users_items = np.concatenate((x_users, x_items), axis=0)\n",
    "\n",
    "    return x_users_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roll(x_users_items, n_users, n_items, n_features):\n",
    "    \n",
    "    \"\"\"Reshape a 1D array of user and item latent\n",
    "    features into their original 2D array format.\n",
    "    Inverse function of `unroll`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_users_items : 1D numpy array\n",
    "        User and item latent features.\n",
    "    n_users : int\n",
    "        Number of users.\n",
    "    n_items : int\n",
    "        Number of items.\n",
    "    n_features: int\n",
    "        Number of latent features to learn. \n",
    "        Determines the overall size of M_users \n",
    "        and M_items.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    M_users : 2D numpy array\n",
    "        Matrix of user latent features. Has\n",
    "        dimensions (n_users, n_features).\n",
    "    M_items : 2D numpy array\n",
    "        Matrix of item latent features. Has\n",
    "        dimensions (n_items, n_features).\n",
    "    \"\"\"\n",
    "    \n",
    "    # retrieve user and item 1D arrays\n",
    "    x_users = x_users_items[0:n_users*n_features]\n",
    "    x_items = x_users_items[n_users*n_features:]\n",
    "    \n",
    "    # reshape 1D arrays into original matrices\n",
    "    M_users = np.reshape(x_users, (n_users, n_features))\n",
    "    M_items = np.reshape(x_items, (n_items, n_features))\n",
    "\n",
    "    return M_users, M_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(x_users_items, Y_true, Lambda, n_users, n_items, n_features):\n",
    "    \n",
    "    \"\"\"Compute cost (error) J from predictions on \n",
    "    Y_true using learned features `x_users_items`. J \n",
    "    is defined as the sum of squared errors plus\n",
    "    regularization penatlies on user and item \n",
    "    latent features.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_users_items : 1D numpy array\n",
    "        User and item latent features.\n",
    "    Y_true : 2D numpy array\n",
    "        Matrix containing true ratings.\n",
    "    Lambda : int\n",
    "        Regularization coefficient.\n",
    "    n_users : int\n",
    "        Number of users.\n",
    "    n_items : int\n",
    "        Number of items.\n",
    "    n_features: int\n",
    "        Number of latent features to learn. \n",
    "        Determines the overall size of M_users \n",
    "        and M_items.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    J : float\n",
    "        Cost associated with prediction on `Y_true` \n",
    "        using learned latent features `x_users_items`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # recover 2D user and item feature matrices\n",
    "    M_users, M_items = roll(x_users_items, n_users, n_items, n_features)\n",
    "\n",
    "    # compute the prediction\n",
    "    Y_predicted = np.dot(M_users, M_items.T)\n",
    "    \n",
    "    # compute the error in the prediction\n",
    "    error = Y_true - Y_predicted\n",
    "    # replace all NaN values with 0\n",
    "    error[np.isnan(error)] = 0\n",
    "\n",
    "    # compute the regularization penalties\n",
    "    User_regularization = (Lambda/2) * np.nansum(M_users * M_users)\n",
    "    Item_regularization = (Lambda/2) * np.nansum(M_items * M_items)\n",
    "\n",
    "    # compute the cost J with regularization\n",
    "    J = (1/2) * np.nansum(error*error) + User_regularization + Item_regularization\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x_users_items, Y_true, Lambda, n_users, n_items, n_features):\n",
    "    \n",
    "    \"\"\"Compute gradient function on `x_users_items`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_users_items : 1D numpy array\n",
    "        User and item latent features.\n",
    "    Y_true : 2D numpy array\n",
    "        Matrix containing true ratings.\n",
    "    Lambda : int\n",
    "        Regularization coefficient.\n",
    "    n_users : int\n",
    "        Number of users.\n",
    "    n_items : int\n",
    "        Number of items.\n",
    "    n_features: int\n",
    "        Number of latent features to learn. \n",
    "        Determines the overall size of M_users \n",
    "        and M_items.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    gradient : 1D numpy array\n",
    "        Gradient of cost J w.r.t user and item\n",
    "        latent features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # recover 2D user and item feature matrices\n",
    "    M_users, M_items = roll(x_users_items, n_users, n_items, n_features)\n",
    "\n",
    "    # compute the prediction\n",
    "    Y_predicted = np.dot(M_users, M_items.T)\n",
    "    \n",
    "    # compute the error in the prediction\n",
    "    error = Y_true - Y_predicted\n",
    "    # replace all NaN values with 0\n",
    "    error[np.isnan(error)] = 0 \n",
    "\n",
    "    # the gradients of user & item features\n",
    "    M_user_gradient = np.dot(error, M_items) + Lambda*M_users\n",
    "    M_item_gradient = np.dot(error.T, M_users) + Lambda*M_items\n",
    "\n",
    "    # reshape gradients into 1D array\n",
    "    gradient = unroll(M_user_gradient, M_item_gradient)\n",
    "\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_users_items, n_users, n_items, n_features):\n",
    "    \n",
    "    \"\"\"Compute prediction on ratings. Predictions\n",
    "    are computed from learned user and item \n",
    "    latent features in `x_users_items`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_users_items : 1D numpy array\n",
    "        User and item latent features.\n",
    "    n_users : int\n",
    "        Number of users.\n",
    "    n_items : int\n",
    "        Number of items.\n",
    "    n_features: int\n",
    "        Number of latent features to learn. \n",
    "        Determines the overall size of M_users \n",
    "        and M_items.\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    Y_predicted : pandas DataFrame\n",
    "        Predictions.\n",
    "    \"\"\"\n",
    "    \n",
    "    # recover 2D user and item feature matrices\n",
    "    M_users, M_items = roll(x_users_items, n_users, n_items, n_features)\n",
    "\n",
    "    # compute predictions from P & Q\n",
    "    Y_predicted = np.dot(M_users, M_items.T) \n",
    "    \n",
    "    # set all negative predictions to 1 (bottom limit)\n",
    "    Y_predicted[Y_predicted < 1] = 1\n",
    "    \n",
    "    Y_predicted = Y_predicted.astype(int)\n",
    "    \n",
    "    return Y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_matrix(Y_true, Y_predicted, average='weighted', labels=[1.0,2.0,3.0,4.0,5.0,6.0]):\n",
    "    \n",
    "    \"\"\"Compute the f1_score between an actual values\n",
    "    matrix and predicted values matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Y_true : 2D numpy array\n",
    "        Matrix of true values.\n",
    "    Y_predicted : 2D numpy array\n",
    "        Matrix of predictions.\n",
    "    average : str\n",
    "        Method for weighting f1-score which is computed\n",
    "        for each label.\n",
    "    labels : list\n",
    "        List of labels to compute f1-scores over.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    f1 : float\n",
    "        f1-score computed using `average` method \n",
    "        across specified `labels`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # get indices of non-NaN values\n",
    "    mask = ~np.isnan(np.array(Y_true))\n",
    "    \n",
    "    # flatten mask into 1D array\n",
    "    mask = mask.flatten(order='C')\n",
    "    \n",
    "    # flatten matrices into 1D arrays\n",
    "    y_true = Y_true.flatten(order='C')\n",
    "    y_predicted = Y_predicted.flatten(order='C')\n",
    "    \n",
    "    # filter the arrays using the mask\n",
    "    y_true = y_true[mask]\n",
    "    y_predicted = y_predicted[mask]\n",
    "    \n",
    "    # compute f1-score\n",
    "    f1 = f1_score(y_true, y_predicted, average=average, labels=labels)\n",
    "    \n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_e(R_train, n_users, n_items, n_features, Lambda, \n",
    "          epochs, alpha, compute_f1=False, seed=42, **kwargs):\n",
    "    \n",
    "    \"\"\"Stochastic gradient descent algorithm. Searches\n",
    "    for the optimal values of user and item latent\n",
    "    features in x_users_items, that minimize the cost J.\n",
    "    Updates are calculated for `epochs` iterations using a \n",
    "    learning rate `alpha`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    R_train : 2D numpy array\n",
    "        Ratings matrix for training dataset.\n",
    "    R_cv : 2D numpy array\n",
    "        Ratings matrix for cross-validation dataset.\n",
    "    n_users : int\n",
    "        Number of users.\n",
    "    n_items : int\n",
    "        Number of items.\n",
    "    n_features: int\n",
    "        Number of latent features to learn. \n",
    "        Determines the overall size of M_users \n",
    "        and M_items.\n",
    "    Lambda : int\n",
    "        Regularization coefficient.\n",
    "    epochs : int\n",
    "        Number of iterations to run.\n",
    "    alpha : float\n",
    "        Learning rate.\n",
    "    compute_f1 : bool (default False)\n",
    "        If true, computes the f1-scores for predictions\n",
    "        on R_train and R_cv at each epoch.\n",
    "    seed : int (default 42)\n",
    "        Seed for numpy's pseudo-random number\n",
    "        generator.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        Nested dictionary containing epoch as keys. Values\n",
    "        associated with each `epochs` are cost J, f1-scores \n",
    "        for training and CV datasets, and optimized \n",
    "        parameters `x_users_items`.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # get cross-validation data if given\n",
    "    R_cv = kwargs.get('R_cv')\n",
    "    \n",
    "    # set random seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # intial random guess of user and item\n",
    "    # latent features\n",
    "    M_users = np.random.rand(n_users, n_features) - 0.5\n",
    "    M_items = np.random.rand(n_items, n_features) - 0.5\n",
    "    \n",
    "    # reshape matrices into 1D array of\n",
    "    # user and item latent features\n",
    "    x_users_items = unroll(M_users, M_items)\n",
    "    \n",
    "    # initialize empty dict to store training results\n",
    "    results = {}\n",
    "    \n",
    "    # loop through `epochs` iterations\n",
    "    for e in range(1,epochs+1):\n",
    "        \n",
    "        # compute the cost\n",
    "        J = cost(x_users_items, R_train, Lambda, \n",
    "                   n_users, n_items, n_features)\n",
    "        \n",
    "        # compute the gradient function\n",
    "        gradient_ = gradient(x_users_items, R_train, Lambda, \n",
    "                      n_users, n_items, n_features)\n",
    "        \n",
    "        # update `x_users_items`\n",
    "        x_users_items = x_users_items + alpha * gradient_\n",
    "        \n",
    "        if compute_f1:\n",
    "            # make prediction\n",
    "            Y_predicted = predict(x_users_items, n_users, n_items, n_features)\n",
    "\n",
    "            # store cost J, f1-scores on training and CV data,\n",
    "            # and the optimized parameters `x_users_items`.\n",
    "            results[e] = {'J': J, 'f1-train': f1_matrix(R_train, Y_predicted), \n",
    "                          'f1-cv': f1_matrix(R_cv, Y_predicted), 'x_users_items': x_users_items}\n",
    "            \n",
    "        else:\n",
    "            # store cost J and optimized parameters `x_users_items`\n",
    "            results[e] = {'J': J, 'x_users_items': x_users_items}\n",
    "        \n",
    "        # print current epoch and cost\n",
    "        print('Epoch %s' % e + ' | ' + 'J : %s' % round(J))\n",
    "        \n",
    "        # logic for stop condition\n",
    "        if e > 2:\n",
    "            # compute delta in previous iteration\n",
    "            delta0 = (results[e-2]['J'] - results[e-1]['J'])\n",
    "            \n",
    "            # compute delta for current iteration\n",
    "            delta = (results[e-1]['J'] - results[e]['J'])\n",
    "            \n",
    "            # if delta for current iteration is larger than\n",
    "            # delta for previous iteration, end updates\n",
    "            if (delta < 0) | (delta > delta0 + 2):\n",
    "                print('Gradient diverging! Ending training...')\n",
    "                break\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # indicate completion and print final J\n",
    "    print('Training complete, final J: %s' % round(results[epochs]['J']))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | J : 188180.0\n",
      "Epoch 2 | J : 187498.0\n",
      "Epoch 3 | J : 186838.0\n",
      "Epoch 4 | J : 186194.0\n",
      "Epoch 5 | J : 185564.0\n",
      "Epoch 6 | J : 184944.0\n",
      "Epoch 7 | J : 184330.0\n",
      "Epoch 8 | J : 183719.0\n",
      "Epoch 9 | J : 183108.0\n",
      "Epoch 10 | J : 182492.0\n",
      "Gradient diverging! Ending training...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "50",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-6fada62bc864>\u001b[0m in \u001b[0;36mSGD_e\u001b[1;34m(R_train, n_users, n_items, n_features, Lambda, epochs, alpha, compute_f1, seed, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;31m# indicate completion and print final J\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training complete, final J: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'J'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 50"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# convert dataframes into numpy arrays\n",
    "R_train_ = np.array(R_train)\n",
    "R_cv_ = np.array(R_cv)\n",
    "\n",
    "# define model parameters\n",
    "n_users = R_train_.shape[0]\n",
    "n_items = R_train_.shape[1]\n",
    "n_features=10\n",
    "Lambda=0.1\n",
    "alpha=0.001\n",
    "epochs=50\n",
    "\n",
    "results = SGD_e(R_train_, n_users, n_items, n_features, Lambda, \n",
    "                epochs, alpha, compute_f1=True, R_cv=R_cv_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(list(results.keys()))\n",
    "y = np.array([results[i]['J'] for i in results.keys()])\n",
    "\n",
    "trace0=go.Scattergl(x=x, y=y, mode='lines+markers')\n",
    "\n",
    "layout=go.Layout(title='Cost Function vs epoch',\n",
    "                yaxis=dict(title='Cost Function'),\n",
    "                xaxis=dict(title='epoch'))\n",
    "\n",
    "fig = go.Figure([trace0], layout)\n",
    "\n",
    "iplot(fig, filename='training.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([i['F1-train'] for i in results.values()])\n",
    "y_cv = np.array([i['F1-cv'] for i in results.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace0=go.Scattergl(x=x, y=y_train, mode='lines+markers',\n",
    "                   name='Train F1-score')\n",
    "trace1=go.Scattergl(x=x, y=y_cv, mode='lines+markers',\n",
    "                   name='CV F1-score')\n",
    "\n",
    "layout=go.Layout(title='F1-scores for Training vs CV datasets',\n",
    "                xaxis=dict(title='epoch'),\n",
    "                yaxis=dict(title='F1-score'))\n",
    "\n",
    "fig = go.Figure([trace0, trace1], layout)\n",
    "\n",
    "iplot(fig, filename='training.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "save_obj(results, 'initial_100epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_t(R_train, n_users, n_items, n_features, Lambda, \n",
    "          epsilon, alpha, compute_f1=False, seed=42, **kwargs):\n",
    "    \n",
    "    \"\"\"Stochastic gradient descent algorithm. Searches\n",
    "    for the optimal values of user and item latent\n",
    "    features in x_users_items, that minimize the cost J.\n",
    "    Updates are calculated until the change (delta) in J, \n",
    "    between iterations, is less than epsilon.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    R_train : 2D numpy array\n",
    "        Ratings matrix for training dataset.\n",
    "    R_cv : 2D numpy array (optional, must be given if\n",
    "    `compute_f1` is True)\n",
    "        Ratings matrix for cross-validation dataset.\n",
    "    n_users : int\n",
    "        Number of users.\n",
    "    n_items : int\n",
    "        Number of items.\n",
    "    n_features: int\n",
    "        Number of latent features to learn. \n",
    "        Determines the overall size of M_users \n",
    "        and M_items.\n",
    "    Lambda : int\n",
    "        Regularization coefficient.\n",
    "    epsilon : float\n",
    "        Training threshold. Training stops when the cost\n",
    "        J is less than epsilon.\n",
    "    alpha : float\n",
    "        Learning rate.\n",
    "    compute_f1 : bool (default False)\n",
    "        If true, computes the f1-scores for predictions\n",
    "        on R_train and R_cv at each epoch.\n",
    "    seed : int (default 42)\n",
    "        Seed for numpy's pseudo-random number\n",
    "        generator.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        Nested dictionary containing epoch as keys. Values\n",
    "        associated with each `epochs` are cost J, f1-scores \n",
    "        for training and CV datasets, and optimized \n",
    "        parameters `x_users_items`.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # get cross-validation data if given\n",
    "    R_cv = kwargs.get('R_cv')\n",
    "    \n",
    "    # set random seed\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # intial random guess of user and item\n",
    "    # latent features\n",
    "    M_users = np.random.rand(n_users, n_features) - 0.5\n",
    "    M_items = np.random.rand(n_items, n_features) - 0.5\n",
    "    \n",
    "    # reshape matrices into 1D array of\n",
    "    # user and item latent features\n",
    "    x_users_items = unroll(M_users, M_items)\n",
    "    \n",
    "    # initialize empty dict to store training results\n",
    "    results = {}\n",
    "    \n",
    "    e = 1 # counter for training iteration\n",
    "    \n",
    "    # large, arbitrary initial value for delta in J\n",
    "    delta = 1000\n",
    "    \n",
    "    # iterate until the delta in J is less than epsilon\n",
    "    while delta > epsilon:\n",
    "        \n",
    "        # compute the cost\n",
    "        J = cost(x_users_items, R_train, Lambda, \n",
    "                   n_users, n_items, n_features)\n",
    "        \n",
    "        # compute the gradient function\n",
    "        gradient_ = gradient(x_users_items, R_train, Lambda, \n",
    "                      n_users, n_items, n_features)\n",
    "        \n",
    "        # update `x_users_items`\n",
    "        x_users_items = x_users_items + alpha * gradient_\n",
    "        \n",
    "        if compute_f1:\n",
    "            # make prediction\n",
    "            Y_predicted = predict(x_users_items, n_users, n_items, n_features)\n",
    "\n",
    "            # store cost J, f1-scores on training and CV data,\n",
    "            # and the optimized parameters `x_users_items`.\n",
    "            results[e] = {'J': J, 'f1-train': f1_matrix(R_train, Y_predicted), \n",
    "                          'f1-cv': f1_matrix(R_cv, Y_predicted), 'x_users_items': x_users_items}\n",
    "            \n",
    "        else:\n",
    "            # store cost J and optimized parameters `x_users_items`\n",
    "            results[e] = {'J': J, 'x_users_items': x_users_items}\n",
    "        \n",
    "        # print current epoch and cost\n",
    "        print('Epoch %s' % e + ' | ' + 'J : %s' % round(J))\n",
    "        \n",
    "        # logic for stop condition\n",
    "        if e > 1:\n",
    "            # compute delta for current iteration\n",
    "            delta = (results[e-1]['J'] - results[e]['J'])\n",
    "            \n",
    "            # if J increases from last iteration (delta < 0)\n",
    "            # end updates and return results\n",
    "            if delta < 0:\n",
    "                print('Gradient diverging! Ending training...')\n",
    "                return results\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        print('Cost delta: %s' % round(delta))\n",
    "        print()\n",
    "        \n",
    "        e += 1\n",
    "    \n",
    "    # indicate completion and print final J\n",
    "    print('Stopping criteria met: delta < epsilon.')\n",
    "    print('Final J: %s' % round(results[epochs]['J']))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current values of (n_features, Lambda, alpha): (50, 0.01, 0.0005)\n",
      "Epoch 1 | J : 199505.0\n",
      "Cost delta: 1000\n",
      "\n",
      "Epoch 2 | J : 197664.0\n",
      "Cost delta: 1000\n",
      "\n",
      "Epoch 3 | J : 195869.0\n",
      "Cost delta: 1794.0\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-d3608a623c36>\u001b[0m in \u001b[0;36mSGD_t\u001b[1;34m(R_train, n_users, n_items, n_features, Lambda, epsilon, alpha, compute_f1, seed, **kwargs)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;31m# compute the gradient function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         gradient_ = gradient(x_users_items, R_train, Lambda, \n\u001b[1;32m---> 81\u001b[1;33m                       n_users, n_items, n_features)\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;31m# update `x_users_items`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-50a0b990e37e>\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(x_users_items, Y_true, Lambda, n_users, n_items, n_features)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m# the gradients of user & item features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mM_user_gradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM_items\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mM_users\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[0mM_item_gradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mM_users\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mM_items\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "R_train_ = np.array(R_train)\n",
    "R_cv_ = np.array(R_cv)\n",
    "\n",
    "grid = {'f': [50, 100],\n",
    "       'L':[0.01, 0.1, 1],\n",
    "       'alpha':[0.0005, 0.001]}\n",
    "\n",
    "epsilon=100\n",
    "\n",
    "evil_master_plan = {}\n",
    "\n",
    "i=1\n",
    "for n_features in grid['f']:\n",
    "    for Lambda in grid['L']:\n",
    "        for alpha in grid['alpha']:\n",
    "            print('current values of (n_features, Lambda, alpha): %s' % \\\n",
    "                  str((n_features, Lambda, alpha)))\n",
    "            \n",
    "            results = SGD_t(R_train=R_train_, n_users=n_users, n_items=n_items, \n",
    "                            n_features=n_features, Lambda=Lambda, epsilon=epsilon, \n",
    "                            alpha=alpha, compute_f1=True, R_cv=R_cv_)\n",
    "            \n",
    "            evil_master_plan[i] = {'f':n_features, 'L':Lambda, 'alpha':alpha, 'results':results}\n",
    "            \n",
    "            i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evil_master_plan.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "for i in evil_master_plan.keys():\n",
    "    save_obj(evil_master_plan[i], 'fullsearch_eps50_run_%s' % i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = min(results.keys())\n",
    "final = max(results.keys())\n",
    "\n",
    "print('F1-score @ epoch %s:' % ini tial + ' %s' % round(results[initial]['F1-train'], 4))\n",
    "print('F1-score @ epoch %s:' % final + ' %s' % round(results[final]['F1-cv'], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = min(results.keys())\n",
    "final = max(results.keys())\n",
    "\n",
    "R_pred_init = predict(results[initial]['xopt'], n_u, n_i, f)\n",
    "R_pred_fin = predict(results[final]['xopt'], n_u, n_i, f)\n",
    "\n",
    "print('F1-score @ epoch %s:' % initial + ' %s' % round(f1(R_train, R_pred_init), 4))\n",
    "print('F1-score @ epoch %s:' % final + ' %s' % round(f1(R_train, R_pred_fin), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1(R_cv, R_pred_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
